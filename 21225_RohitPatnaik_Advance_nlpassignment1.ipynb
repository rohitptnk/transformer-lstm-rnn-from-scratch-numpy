{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Assignment 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "335b5e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32064</th>\n",
       "      <td>35863</td>\n",
       "      <td>80815</td>\n",
       "      <td>Los Angeles / Ventura County</td>\n",
       "      <td>07-04-2020</td>\n",
       "      <td>Whoa! Hadn't heard about this until now.\\r\\r\\n...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>6198</td>\n",
       "      <td>51150</td>\n",
       "      <td>North Charleston, SC</td>\n",
       "      <td>17-03-2020</td>\n",
       "      <td>SC CORONAVIRUS UPDATE | If you've been out &amp;am...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27898</th>\n",
       "      <td>31697</td>\n",
       "      <td>76649</td>\n",
       "      <td>#USA</td>\n",
       "      <td>04-04-2020</td>\n",
       "      <td>I just gave away 6000 of other people s money ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31017</th>\n",
       "      <td>34816</td>\n",
       "      <td>79768</td>\n",
       "      <td>Wellington City, New Zealand</td>\n",
       "      <td>06-04-2020</td>\n",
       "      <td>Kaikohe locals queue for covid 19 testing afte...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34738</th>\n",
       "      <td>38537</td>\n",
       "      <td>83489</td>\n",
       "      <td>Whangarei, New Zealand</td>\n",
       "      <td>08-04-2020</td>\n",
       "      <td>Covid 19 coronavirus: Hundreds request virus c...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName                      Location     TweetAt  \\\n",
       "32064     35863       80815  Los Angeles / Ventura County  07-04-2020   \n",
       "2399       6198       51150          North Charleston, SC  17-03-2020   \n",
       "27898     31697       76649                          #USA  04-04-2020   \n",
       "31017     34816       79768  Wellington City, New Zealand  06-04-2020   \n",
       "34738     38537       83489        Whangarei, New Zealand  08-04-2020   \n",
       "\n",
       "                                           OriginalTweet Sentiment  \n",
       "32064  Whoa! Hadn't heard about this until now.\\r\\r\\n...  Negative  \n",
       "2399   SC CORONAVIRUS UPDATE | If you've been out &am...  Positive  \n",
       "27898  I just gave away 6000 of other people s money ...  Negative  \n",
       "31017  Kaikohe locals queue for covid 19 testing afte...  Positive  \n",
       "34738  Covid 19 coronavirus: Hundreds request virus c...  Positive  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Corona_NLP_Train.csv\", encoding=\"latin1\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8a654f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neutral' 'Positive' 'Extremely Negative' 'Negative' 'Extremely Positive']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "class_labels = df[\"Sentiment\"].unique()\n",
    "print(class_labels)\n",
    "print(len(df[\"Sentiment\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d0fe116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Extremely Negative\n"
     ]
    }
   ],
   "source": [
    "label_to_index = {}\n",
    "index_to_label = {}\n",
    "\n",
    "for i, label in enumerate(class_labels):\n",
    "    label_to_index[label] = i\n",
    "    index_to_label[i] = label\n",
    "\n",
    "print(label_to_index[\"Extremely Negative\"])\n",
    "print(index_to_label[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "428a6618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'challenging', '#nohandsanitizer', '2019,', 'resulted', 'several', 'store,', 'after', 'freezer', 'themselves.', 'anyone', 'deem', 'but', 'die', 'entire', \"They're\", 'news', 'employer', 'got', 'took', 'Thank', '(at', 'weÂ\\x92ve', 'attendees,', 'advantage', '&amp;', 'https://t.co/PnA797jDKV', 'off', 'my', 'morning', 'actions', 'My', 'necessities', '#CoronaVirusUpdates', 'shut', 'misinformed', 'mean', 'safty', 'ensues', 'whatever', 'trolleys', 'objected', '#manila', 'leave.', \"Didn't\", 'An', 'feel', 'these', '#totallockdown', 'BE', 'provide', 'still', \"General's\", 'restaurants,', '@SenatorRomney', 'things', 'supplies,', 'form', 'sound', 'health', 'find', '@Tim_Dodson', '4-12', 'redrawn', 'https://t.co/cfXch7a2lU', 'store.', 'he', 'fact', 'for', 'result', 'cleaning', 'systems', 'watchdog', 'retail&amp;a', 'put', 'normal!', 'sometimes', 'this,', 'https://t.co/3hCd9IiWoX', 'looked', 'lol', 'capitalizing', 'easy.', '#Liverpool', 'https://t.co/dCSXHUj3U0', '#bestiptv', 'Worried', '??', 'just', 'orange', 'paper', 'cheese.', 'through', 'can.', 'lines', 'frozen', 'WILL', 'pick', \"You're\", 'Blues', 'https://t.co/pzirO10avf', 'advice', 'back', 'previous', 'has.', 'share', 'virus.', 'them,', 'right', 'https://t.co/WtB0B1AMON', 'hardship', 'goverments', 'La', 'COVID-19,', 'walk-in', '@patel4witham', '#confinement', 'quickly.', '65', 'need.', 'numbers', 'continued', '(food,', 'tours', 'What', 'coughing', 'Just', 'laying', 'area', 'patches', 'selfish', 'illness', 'close', 'cough', '#ipTV', 'prevent', 'government', 'mac', 'interest/fees)', 'crisis', '#coronapocolypse', 'NYC?', 'traveled', '#Coronavid19', 'CoVid-19.', 'class', 'https://t.co/4WnrrK9oKC', 'structure,', 'global', 'lives', 'often', 'markets', 'please,', 'debts', 'sparking', 'pictures', '(via', 'if', 'being', 'selfish.', \"aren't\", 'towels,', '#Notoiletpaper', 'care', 'rumours', '#horningsea', 'WeÂ\\x92ve', 'thank', 'I', 'frankly', 'tub', 'paranoid,', 'Up-To-Date', 'afraid', 'customer', '\"Both', 'eat?', 'https://t.co/EbEnURmmJS', 'Stores', 'remain', 'alternatives', 'pair', 'loved', 'sanitizer,', 'testing', 'COVID-19.', 'front', 'caring', 'hate', 'wont', 'realized', \"won't\", '&gt;&gt;', 'supermarket', 'week,', 'https://t.co/ZHbh898lf6', 'kind', 'No', 'called', 'strips', 'Yes', 'because', 'continue', 'serious', 'within', '#????_????', 'reducing', 'https://t.co/YR2tcPx1Ee', 'Apologies', '#onlineshopping', 'relief', 'seeing', 'complaint', 'found', 'A', 'Picture', 'https://t.co/dQox6uSihz', '#Monthly', 'actively', 'exec', 'juice,', 'The', '#HD', 'NYC', 'fight', 'Pretty', 'scale', 'Tragic.', 'test,', '?????', 'Country', 'but.', 'that', 'since', 'not', 'puzzle', 'needs', 'can', 'Plus', 'possible', 'so,', 'thanked', 'priority.', 'Could', 'might', 'Story:', 'So', 'chinese', 'Yeah', 'Behavior', 'staff', '@eyeonthearctic', 'great', 'shops', 'https://t.co/sk9qCJsnYl', 'Me,', 'go...', '\"shielded\"', 'bill', 'life.', 'please', '#COVID19', 'closed...\"', '@TinaMcCauley70', 'Cal', 'Stymies', 'meat,', \"it's\", '453,000', 'https://t.co/6nGNFJmy89', 'food-delivery', 'fact:', 'Â\\x93As', 'Supermarkets', 'pragmatic.', '(DCWP)', 'weÂ\\x92re', 'Sadly', '(some', 'products;', 'rises', 'Taking', '@kroger', 'panic,', 'water...', 'this', 'water.', 'hits', 'Why', '\"Everything', 'sign', 'buying', 'wash', 'collections', \"don't\", 'inflated', 'disabled', 'a', 'Insights', 'under', 'However,', 'wages', '#Confinementotal', 'stupid', 'worry', '#Malaysia', 'https://t.co/GDDPTudCvj', 'lot', 'well', 'particularly', '#IMadeThisUp', 'common', 'https://t.co/19uNybttHl', 'lockdown).', 'Here', 'insufficient', 'THANK', '101', 'March', '#Coronavirus', 'abou', 'https://t.co/ztN3iMkgpD', 'disciplined?', 'high', '#CoronavirusReachesDelhi', 'her', 'something', 'conspiracy', 'tips', 'staying', 'ones,', 'others:', 'demand.', 'omnichannel', '\"dumb\"', 'sense', 'everywhere,', 'following', 'causes', 'closed', 'biggest', 'Changed', 'future,', 'week', 'diarrhoea,', 'guilde', '#omnichannel', 'distancing', 'understanding!', 'reimbursement?', 'products.', 'THE', 'sick', 'ago.', 'safe.', 'answered', 'FOOD', '#JhalakBollywood', 'already', 'classroom', 'notes.', 'consumer-debt', 'useful.', 'demostrated', 'https://t.co/StTAkyqQiZ', 'spooked', 'symptoms', 'middle', 'above', '(over', 'marketing', 'empty.', 'chain', 'major', 'think', 'Talk', 'startups', 'Amazon', 'here:', 'despicable', 'sure', 'set', 'Covid-19', 'worst', '@Loreign83', 'you???', '#iptvnew', 'providing', '#pandemic', '#Philippines', 'surging', 'fruit,', 'judged!', 'Bought', 'all,', 'company', '#Breaking', 'additional', 'events.', 'DidnÂ\\x92t', 'create', 'photos', 'hygiene', 'interrupted.', 'loan', 'https://t.co/HxWs9LAnF9', '#govindia', 'Their', 'have', 'show', 'empty', 'lacking).', 'system', 'was', 'covid', 'Coronavirus,', \"There's\", 'or', 'confirmed', 'deliveries', '70).', 'Lines', '@MeNyrbie', 'Worker', 'say', 'donÂ\\x92t', 'disinfectant&amp;clean', 'Covid-19.', 'freezable', 'point', '#iptv2020', 'adopts', 'opposed', 'exchange', '#Yearly', 'You', 'https://t.co/ld05k5Eyns', 'Facebook', 'time.', 'side', 'https://t.co/zrlG0Z520j', 'posting', 'https://t.co/lsGrXXhjhh', 'safety', 'requested', 'statement.', 'next', 'counties', 'wellbeing', 'placed', 'Crisis', 'South', 'supplied', '#coronavirus', 'rationing', '19.', 'try', 'flocked', 'CEO', 'disgusting', 'it', '#food', '@my_amigouk', 'risky', 'Lives', 'outbreak', 'ready', 'prevention,we', 'nonprofit.', 'healthy', '#ecommerce', 'https://t.co/usmuaLq72n', 'https://t.co/kgyDow3Nrz', 'therefore', 'closing', 'disappointment', '#walmart', 'know', '#nofood', '#digital', 'challenges.', 'commented', 'https://t.co/hQrYRNXFhv', 'Â\\x93Today', 'test', 'VIDEO', 'saw', 'avoiding', 'Was', 'https://t.co/Hrbzmh95VQ', 'banks', 'kid', 'been', 'nicest', 'US!', 'Heard', 'complaint.', '#Football', 'Went', '...images', '-', 'virus?', 'with)', 'capitalism', 'be', 'spread', 'Protect', 'holding', 'delivery', 'urge', '#StopPanicBuying', 'https://t.co/Pr8NpKX41A', 'order', 'ethical', 'there', 'spent', 'until', 'To', '#seafood', 'viral,', 'sends', 'waived', 'made', 'potential', 'talking', 'Russia', 'tested', 'we', 'scare', 'cures\"', 'goods,', 'may', 'positive', 'struggling', 'thier', 'month', 'organizations', 'sixty', 'word\"Overcharge\")', 'ELISA', 'weeks,', 'food,', 'amid', 'Hi,', '@7SealsOfTheEnd', 'open', '@afneil', 'link', 'prices', 'malls', 'their', 'from', 'ADARA', 'digitally', 'Resource', 'submit', 'Closed', 'unlawful', 'shortage,', 'spread,', '\"observed\"', 'panic', 'CLERK!', 'Australia:', 'now', 'coronavirus.', '#Iptv', '(3', 'Releases', 'battling', 'ENOUGH', 'https://t.co/kw91zJ5O5i', 'isolation', 'discouraged?', '#peoplearelosingtheirminds', 'vegetables', 'special', 'spirit!', 'impacts', 'etc.?', 'orders', 'without', 'situation', 'your', 'https://t.co/yDGX4lK8L0', 'Well', 'happening!', 'criminals', 'slashed', 'customers', 'malicious', 'require', '#StayHealthy', '?', 'Low', 'cannot', 'both', 'https://t.co/KT5J4QqCwS', '#StopTheMadness', '#JhalakTollywood', 'home.', 'Of', 'orders,', 'afford', 'to', '100', 'offered', 'https://t.co/WMqR8QWoiG', 'public,', 'either', 'Help', 'essentials', '#services', '#COVID-19', 'matrices', '#Prices', 'ATM,', 'n', '13', 'clothes.', 'up', '/', 'sharing', 'https://t.co/krTCGiUHQS', 'checkouts', 'somebody', 'protocols', 'see', 'There', 'done', 'These', 'necessary!', 'Daily', 'left', 'jobs.', 'percent', 'crap.', 'china', 'Fears', 'outbreak.', 'key', 'work', 'experience', 'cannabis', 'current', 'agoing', 'inficted', 'guilty', 'regionÂ\\x92s', 'thoughts', 'Consumers', 'crowding', 'idea', 'who', 'weary', 'Advantage', 'process', 'in.', 'services', 'need', 'medical', 'shopping,', 'BS,', 'speakers', 'County', 'eating', 'said', 'due', 'spray', 'some', 'live', '....', 'products', 'closures', 'potatoes,', 'FULLY', 'older', 'workers', 'clean', 'payment', 'THERE', 'Now', 'IÂ\\x92m', 'personnel', 'forget', 'increased', '#Rebel', '@RetailX', 'We', 'community.', '?????????', 'Tech', 'quack', '#houston', '#stoppanicbuying', 'capacity', '4.', 'responders?', 'Even', 'Somehow', 'Foods...', 'But', 'medicines,', 'toilet', 'die/get', 'budgets', 'moving.', 'individuals', 'IPTVLinks', '...Â\\x93at', 'clothes', 'Consumer', '#restezchezvous', 'day.', 'times', 'decisions.', 'self', '#retail', 'Breaking', 'sees', \"I've\", 'parents,', '#?????????', \"people's\", 'panic.', 'exposed', 'an', 'basic', 'longer', 'unable', '16', 'nations', 'DonÂ\\x92t', 'Raid', 'via', 'skyrocketing', '(and', 'up.', '#IndiaFightsCorona', 'https://t.co/WnQSoMtkVI', 'prefer', 'checkout', 'yesterday,', '@Target', 'calling', 'https://t.co/Ybg8Zupdf6', 'way', 'saved', 'emergency,', 'panick.', 'employees', 'Department', 'Let', 'less', 'capable', 'From', '#trends', 'Protection', 'sanitizers', 'stay', 'CHECK', 'restaurants', 'hand', '#panicbuying', 'at', 'NATIONWIDE', 'calm', \"can't\", 'mysterious', 'the', 'our', '7-9', 'situation,', 'explode', 'Will', 'Coronavirus', 'supermarket.', 'times.', 'Customers', 'those', 'safe', '#CoronaVirus', '#StockUp&amp;LockUp', 'utility', \"hasn't\", 'supplies', 'effected', 'stock-pile', 'eyes', 'shortage...', 'corona', 'Habra', 'me', 'yet', 'Civics', 'cause', 'They', 'phone', 'hard', '#Cheap', 'anything', 'are', 'know.', '#LockdownCanada', 'once-obscure', 'against', '#flu', 'last', 'over', 'Chinese', 'Arctic', 'whether', 'Resonable', 'retail', '#CoronavirusFrance', 'panicking.', 'results', 'downward', 'dad', 'remains', '@joncoopertweets', 'itÂ\\x92s', '#socialdistancing', 'response', 'Lost', 'few', 'etc', 'run', 'virus', 'around', 'PLEASE,', '#CoronaVirusSA', \"It's\", '#DontPanicBuy', 'exposure', 'in', 'irosponcible', '2', 'sales,', 'low,', '@pymnts', 'Living', 'Glitch', 'FREE', 'dedicated', 'precipice', \"'Hole'\", 'pandemics;', 'group', 'DEALS!', 'rise', 'Morning', 'is', '#covid?19uk', 'completely', 'Pausing', 'Same', 'everything', 'regular', 'wait', 'manage', '#QuarantineLife', 'yet!', 'shoes?', 'https://t.co/9G3kgqIXJ8', 'Bringing', 'goods', 'house.', 'mobile', 'Insight', 'suspend', '16MAR20', 'operation,', 'https://t.co/Lg7HVMZglZ', 'probably', 'GROCERY', 'keep', 'outcome.', 'shelve', 'only', '#BeKind', 'items', 'stop', 'hoping', 'https://t.co/bInCA9Vp8P', 'preparing', 'https://t.co/ieFDNeHgDO', '2K', 'leave,', 'chemicals', 'extraordinary', 'contact', 'beans.', 'Fresh', 's', 'rules', 'you', 'half', 'published', 'all', 'bread', 'https://t.co/GB11EFBYIB', 'staffing', 'Shafwan', 'surprise', 'us', 'classes', 'much', 'thatÂ\\x92s', 'months', 'It', 'lose', '#COVID?19', '@BorisJohnson', 'little', 'breed', 'own', 'pandemic', 'In', 'other', 'https://t.co/48nG14me6E', 'packages', 'Global', '\"isolation\"', '#toiletpapercrisis', 'paper.', 'Retail', 'epidemics', 'house', 'canÂ\\x92t', '#Collapse', 'Stay', '#jlmcobrand', 'thinking', 'curtail', 'time,', 'into', 'kids,', 'herbs', 'Has', 'Cashier', 'ID.', 'lost', 'dry', 'chemist', 'impacting', 'Grocery', 'left.', 'needed', 'exception.', 'supermarkets', 'increases', 'protect', 'So,', \"you're\", 'See', 'isolated', 'UK', 'clerk.', 'stocked', 'whole', 'too', 'curbside', 'introduce', 'https://t.co/5CecYtLnYn', 'masks', 'weekend,', 'Monday,', '...', 'cancelled', 'shopping.', 'word', 'also', 'resulting', 'implemented', 'Trends', 'social', 'gone.', 'poll', 'empty...', 'gives', 'out', 'alternative?', 'guidance', '@10DowningStreet', 'were', 'As', 'Travel', 'mum', 'outside', 'buckle', 'play', 'papers.', 'office:', 'team', 'stocking', 'charging', 'blows', 'recommending', 'according', 'FAQs', 'Montgomery', 'groceries,Â\\x94', 'Mind', 'dependent', 'Our', 'should', 'deliver', 'quarantine', 'affecting', 'his', 'money', 'any', '10', 'Curious,', '#alert', 'maybe', 'will', '#JhalakKollywood', 'poss', 'unpredictable,', '\"idiots.\"', 'went', 'why', 'many', 'use', '#nameandshame', 'so', 'events,', 'mandatory', 'AMAZING', 'flour,', 'student', 'https://t.co/MdMmoBttOP', 'Where', 'interest', 're-stocked', 'includes', 'Click', '&gt;', '#BreakingNews', 'Seen', 'everyone', 'Create', 'letter', 'single', 'bank', 'came', 'stuff', 'theories,', 'accounts', 'first', 'insights', '@Chrisitv', 'calm,', 'same', 'with', 'spokeswoman', 'neighborhoods', 'spend', 'pain', 'dwindling', 'stocked,', 'come', 'Reports', '#covid2019', 'Zaidon', 'load', 'shopping', 'anymore.', 'world', 'crazy.', 'meds', 'where', 'This', '@FinFabUK', 'Brands', 'making', 'Soon', 'packaged', 'Australians,', 'deep', 'focus', 'supply', 'head', 'Having', 'food', 'pat', 'which', 'Kajang', '#Service', 'normally', 'https://t.co/iFz9FAn2Pa', 'immediate', 'flexibility', 'plan:', 'village', 'weeks', 'local', 'shoppers', 'disgraceful', 'thereÂ\\x92s', '@BobJLowe', 'Attorney', 'poses', 'COVID-19', 'it,', 'back!?', 'Mum,', 'addition', 'businesses', 'communities.', 'today', 'aisle', 'Sullivan', 'working', '#lockdown', 'event', 'could', 'alarming', 'consumer', 'Is', 'cause.', '#covid19', 'sorry', '#CovidNYC', 'runways', '@balajis', 'works', 'proliferation', '#StayAtHome', 'plan!', 'CHEAP', '@CNBC).', '#Malaysia2020', 'time', 'amount', 'toxic', 'companies', 'entrance', '#shoponline', 'days.', 'hoard', 'https://t.co/T7qejP3hys', 'impact', 'school', 'ship', 'study.', 'USA', 'using', '#BrickAndMortar', 'possibly', '#consumer', 'crisis.', 'With', 'https://t.co/JMZZJOmNT9', 'home', 'now:', 'offers', 'thrust', 'https://t.co/riNKwskeRS', '2020', 'On', 'them', 'shows', 'support', 'surveillance', 'Also', '????', 'hiring', 'issues', 'parents', 'https://t.co/bPodDdPRcE', 'thing,', 'followed', 'Told', '#economy', 'so.', 'thought', 'doors,', '#Trials', 'Hunger', 'stores', 'someone', 'immune', 'look', 'Mumbai', \"they're\", 'details', 'white', 'food.', 'negative', 'economic', 'distillery', 'reported', 'here', 'amp', 'Malaysia', 'reports', 'give', 'important.', 'Coalition', '#ipTv', 'economy.', 'Due', 'https://t.co/K3uJlcjqDB', 'PYMNTS', 'demand', 'https://t.co/zr67d1u12Q', 'expect', 'fun', 'themselves', 'page', 'means', 'healthy.', 'neighbours', 'Find', 'bc', 'expenses', 'absolutely', 'buy', 'and', 'beef', 'like', 'worth', 'starvation', 'stock,', 'significant', 'far', 'business', 'Never', 'paid', 'elderly,', 'functions', 'line', 'help', 'restrictions', 'dive', '#NoHandShakes', 'ItÂ\\x92s', 'pressures', '???????', 'halting', 'fear,', 'https://t.co/eVXkQLIdAZ', 'man', 'hours', 'situation.', 'as', 'updated', 'nothing', 'doesnÂ\\x92t', 'Iran', 'young', 'ensure', 'Do', 'stuff?', 'Supermarket', 'had', 'Atlanta', 'works.', 'over.', 'finances?', \"#covid19's\", '16.', 'worried', 'If', 'Thanks', 'than', 'am', 'pressure.', 'richest', 'Know', 'goverment', 'stopping', 'comes', 'gonna', 'void', 'environment', 'melt-blown', '12', 'pretty', 'prove', 'EVERYONE', 'meals', 'case', 'beginning', 'About', 'list', 'Not', 'IÂ\\x92ve', 'does', 'DO', 'Love', 'nonprofit', 'sea-food', 'cost,', 'leave', 'on', 'easily', 'Dear', '#Covid_19', 'ONLY', 'when', 'fresh', 'packs.', 'you.', '#COVID19france', 'purchase', 'adequate', 'free', '#covid_19', 'file', 'shop;', 'full', '@Phil_Gahan', 'Online', 'Deliveries', 'https://t.co/lYZg2kfsm0', 'Share', 'COVID-19?', 'https://t.co/oEx6Y8mm2K', 'Sadly,', 'months).', 'https://t.co/ncTXF8TGyf', 'reduce', 'people', 'increase', '#Subscriptions', 'public', 'they', 'doing', 'methods', 'https://t.co/3jKK3CqXfQ', 'necessary..', 'of', 'preparation', 'days', 'shelves', 'sweet', 'https://t.co/xX6ghGFzCC', \"I'm\", 'bogglingly', 'such', 'about\".', 'receive', '(use', '#FakeNews', 'https://t.co/1C1cMLmQii', 'two,', 'what', 'top', 'States.', 'https://t.co/51bL8P6vZh', 'All', 'tp.', 'ok,', '#MUFC_Family', 'Flourish', 'higher', 'decisions', 'couple', 'schools', 'bought', 'physical', 'soups', 'rolls', 'online', '19', 'best', \"we're\", 'market', 'https://t.co/0CV0793olS', 'makes', '#iptvdeals', 'attempts', 'payments', 'must', 'saying', 'hope', '@peanut_astro', 'When', 'https://t.co/I2NlzdxNo8', 'tech', 'would', 'Covid-19,', 'more', 'County,', 'scenario,', 'seen', 'https://t.co/5Z24hptT9M', 'Covid', 'Me:', 'siblings,', 'essential', 'folks', 'about', 'offering', 'PLEASE!', 'price', 'pal', 'Check', 'struggling,', 'important', '#Cinema', 'Instead', 'https://t.co/TbzZ2MC3b3', '??????', 're', 'av', 'police', 'elderly', 'Feel', 'payer', 'United', 'humans', 'Center', 'today.', 'wife', 'always', 'support:', 'going', \"Y'all\", 'hypermarket', '#hotmovies', 'coronavirus', '#Movies', 'store', 'across', 'chains', 'canned', 'DM', 'ALL', 'opportunity', 'buy;', 'intensified', 'by', 'Whole', 'though', 'First', 'https://t.co/8YWaKFjExC', 'YOUR', 'https://t.co/0uHGM8gsp8', 'world,', 'forming', 'facing', 'Yourself', '#COVID2019usa', 'pandemic...', 'March).', 'Woolworths', 'yourself', 'CN', 'emergency', '\"I\\'m', 'extreme', 'sugar,', \"I'll\", 'Corner:', 'extra', '@WorldHealthOrg2', 'how', 'no', 'while', 'community!', 'retailers', 'Yes,', '@TheJoshuaTurner', 'pressure', 'material', '@grantshapps', '@US_FDA', '#delivery', '#cdc', '#Adult', 'litteraly', 'FOR', 'complex', 'alr', 'COVID', 'LetÂ\\x92s', '???', '#tracker...', 'MD.', 'online.', 'serve', 'further', 'wanted', 'seniors', 'massive', 'geographies', 'Brands:', 'increasing', 'fair', 'Corona', 'near', 'one', 'credibility', '#COVID19Aus', 'Amazon:', 'Blip', '#jlmco', 'Panicky', 'GP', 'breaking', 'workers.', '.@kroger', 'closed,', '#COVID_19', 'https://t.co/1ksn9Brl02', 'Tracker', '#mufc', 'patience', 'stock', '19?.', 'go', 'Foods,', '#iptv', \"We're\", 'frank', 'totally', 'associates', 'before', 'https://t.co/AVKrR9syff', 'household', 'People', 'lengthen', 'she', 'below:', 'https://t.co/9idZSis5oQ', '@WSJ', 'get', 'Confidence', 'fabric.\"', 'How', 'coming', 'behavior,', '1.', 'has', 'frontline', 'majority', 'sanitizing', 'accumulation', 'https://t.co/U5Dg3LoFYG', 'purchased', 'Africans', 'family', 'Everyone', 'issues.', 'Dont', 'Shopping', '#ConfinementGeneral', \"I'd\", '#COVID2019', 'consider', 'however', 'whinge', 'take', '#???_????????', 'jobs,', 'do', '??No', 'hosting', 'cash', 'https://t.co/g5UZn06gb6', 'two', 'especially', 'indicates', 'Please', 'really', 'donating', '#stayhome', 'And', \"what's\", '300', 'grocery', 'during', 'feeling', 'punitive', 'life', 'yesterday', 'new', 'etc.),', '#18Movies', 'For', 'etc.', 'profiteer', 'racism,', '#CoronavirusOutbreak', 'community', '#Scammers'}\n",
      "1590\n"
     ]
    }
   ],
   "source": [
    "tweets = list(df[\"OriginalTweet\"][:100])\n",
    "# print(tweets[0:5])\n",
    "vocab= set(\" \".join(tweets).split())\n",
    "print(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "548de5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "token_to_word = {}\n",
    "word_to_token = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    token_to_word[i]=word\n",
    "    word_to_token[word]=i\n",
    "\n",
    "print(word_to_token[\"several\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "23e545de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please don't hoard food and water. There's absolutely no need to panic buy; the supply chain is completely interrupted. And above all, please don't hoard sanitizing products; there are people out there who really need them, probably more than you. #DontPanicBuy #coronavirus\n"
     ]
    }
   ],
   "source": [
    "sample_input = tweets[88]\n",
    "print(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "129de3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1567, 293, 1141, 1085, 1220, 285, 415, 1218, 1461, 658, 603, 560, 1433, 768, 1082, 371, 851, 853, 402, 1571, 367, 393, 259, 293, 1141, 1540, 278, 513, 800, 1325, 980, 513, 652, 1568, 658, 106, 873, 1385, 1267, 1305, 832, 452]\n",
      "[1567, 293, 1141, 1085, 1220, 285, 415, 1218, 1461, 658, 603, 560, 1433, 768, 1082, 371, 851, 853, 402, 1571, 367, 393, 259, 293, 1141, 1540, 278, 513, 800, 1325, 980, 513, 652, 1568, 658, 106, 873, 1385, 1267, 1305, 832, 452, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "(50, 1)\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = []\n",
    "for word in sample_input.split():\n",
    "    tokenized_input.append(word_to_token[word])\n",
    "\n",
    "\n",
    "print(tokenized_input)\n",
    "while len(tokenized_input)<50:\n",
    "    tokenized_input.append(0)\n",
    "\n",
    "print(tokenized_input)\n",
    "tokenized_input = np.array(tokenized_input).reshape(-1,1)\n",
    "print(tokenized_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7c4adb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please don't hoard food and water. There's absolutely no need to panic buy; the supply chain is completely interrupted. And above all, please don't hoard sanitizing products; there are people out there who really need them, probably more than you. #DontPanicBuy #coronavirus challenging challenging challenging challenging challenging challenging challenging challenging\n"
     ]
    }
   ],
   "source": [
    "reconstructed_input = []\n",
    "tokenized_list_input = tokenized_input.flatten().tolist()\n",
    "# print(tokenized_list_input)\n",
    "for token in tokenized_list_input:\n",
    "    reconstructed_input.append(token_to_word[token])\n",
    "\n",
    "reconstructed_input = \" \".join(reconstructed_input)\n",
    "print(reconstructed_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a78857cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b1076ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 50\n",
    "hidden_dim = 50\n",
    "output_dim = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff6fa9",
   "metadata": {},
   "source": [
    "1. h0 (init 0)\n",
    "2. for i in range(len_input):\n",
    "3. $h_i$ = $tanh(W*inputEmbedding + U*h_{i-1})$\n",
    "4. y = tanh(V*h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e397fac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2477219  0.56088804 0.2294584  ... 0.97798683 0.05999492 0.77490627]\n",
      " [0.28783237 0.29851048 0.70043653 ... 0.80917525 0.06637459 0.56640805]\n",
      " [0.72777089 0.19971296 0.55748764 ... 0.76240091 0.47685784 0.23391781]\n",
      " ...\n",
      " [0.95672741 0.27685078 0.46049392 ... 0.25786418 0.52374716 0.43249829]\n",
      " [0.30017212 0.85438438 0.59811774 ... 0.24418652 0.72106694 0.48809714]\n",
      " [0.95127098 0.34629157 0.73388608 ... 0.110792   0.26911229 0.76245162]]\n",
      "(50, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "W = np.random.rand(hidden_dim, input_dim)\n",
    "U = np.random.rand(hidden_dim, hidden_dim)\n",
    "V = np.random.rand(output_dim, hidden_dim)\n",
    "print(W)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "45c9258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_fn(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7bdf191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n",
      "(50, 50)\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input.shape)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c35e35bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14803.23424206]\n",
      " [18181.1171005 ]\n",
      " [19325.31034023]\n",
      " [21757.23764413]\n",
      " [16041.26277987]\n",
      " [19785.29978655]\n",
      " [17399.05778636]\n",
      " [21630.66287055]\n",
      " [17860.41515219]\n",
      " [19357.26497123]\n",
      " [17712.13446535]\n",
      " [16703.54963972]\n",
      " [18020.28392919]\n",
      " [20361.72355336]\n",
      " [14387.12521511]\n",
      " [18823.42191155]\n",
      " [14817.40958827]\n",
      " [18074.57238141]\n",
      " [19796.01898248]\n",
      " [21525.46744195]\n",
      " [17060.27308461]\n",
      " [16879.22439678]\n",
      " [17095.33118772]\n",
      " [18047.28969923]\n",
      " [17405.44303607]\n",
      " [19309.85665089]\n",
      " [20214.52378502]\n",
      " [17703.7305956 ]\n",
      " [12225.65102972]\n",
      " [17062.69573009]\n",
      " [16260.5220199 ]\n",
      " [19592.77227586]\n",
      " [16027.86013308]\n",
      " [17932.45034862]\n",
      " [19329.13748521]\n",
      " [17280.391125  ]\n",
      " [15946.22716145]\n",
      " [19778.17059999]\n",
      " [17409.33706074]\n",
      " [16387.6440207 ]\n",
      " [18393.99052693]\n",
      " [17033.1264692 ]\n",
      " [19310.77905142]\n",
      " [18962.99433364]\n",
      " [18258.48322031]\n",
      " [20138.19517077]\n",
      " [19341.27415841]\n",
      " [16855.97048578]\n",
      " [17519.35456828]\n",
      " [20900.4257829 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = activation_fn(W@tokenized_input)\n",
    "print(h1)\n",
    "np.shape(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "75e913d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5496df5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[426946.81043085]\n",
      " [471860.59028371]\n",
      " [410871.24545725]\n",
      " [454743.86729878]\n",
      " [471506.26168746]]\n"
     ]
    }
   ],
   "source": [
    "y1 = activation_fn(V@h1)\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5c6555c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95474153 1.05517804 0.91879323 1.0169015  1.05438569]\n"
     ]
    }
   ],
   "source": [
    "list_output = np.array(y1.flatten().tolist())\n",
    "scaled_arr = list_output / np.mean(list_output)\n",
    "print(scaled_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b76d0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "240c3a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72206774 0.74176799 0.71479615 0.73436861 0.74161618]\n"
     ]
    }
   ],
   "source": [
    "class_probs = sigmoid(scaled_arr)\n",
    "print(class_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "19fa93a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "max_value_index = class_probs.tolist().index(max(class_probs))\n",
    "print(max_value_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d8665a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "output_class = index_to_label[max_value_index]\n",
    "print(output_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a5526",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba45d59",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9b48a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9514</th>\n",
       "      <td>13313</td>\n",
       "      <td>58265</td>\n",
       "      <td>Email: richard@coilpod.com</td>\n",
       "      <td>20-03-2020</td>\n",
       "      <td>#Restaurants in these #coronavirus times can s...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>5094</td>\n",
       "      <td>50046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17-03-2020</td>\n",
       "      <td>@Reuters #COVID2019 Stock up now! \\r\\r\\n   You...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24163</th>\n",
       "      <td>27962</td>\n",
       "      <td>72914</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>26-03-2020</td>\n",
       "      <td>Lysol Laundry Sanitizer Additive Crisp Linen -...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22002</th>\n",
       "      <td>25801</td>\n",
       "      <td>70753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25-03-2020</td>\n",
       "      <td>Interesting. The sub-saharan story is incomple...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10475</th>\n",
       "      <td>14274</td>\n",
       "      <td>59226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20-03-2020</td>\n",
       "      <td>This is absolutely THE ONE</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName                    Location     TweetAt  \\\n",
       "9514      13313       58265  Email: richard@coilpod.com  20-03-2020   \n",
       "1295       5094       50046                         NaN  17-03-2020   \n",
       "24163     27962       72914                New York, NY  26-03-2020   \n",
       "22002     25801       70753                         NaN  25-03-2020   \n",
       "10475     14274       59226                         NaN  20-03-2020   \n",
       "\n",
       "                                           OriginalTweet           Sentiment  \n",
       "9514   #Restaurants in these #coronavirus times can s...  Extremely Negative  \n",
       "1295   @Reuters #COVID2019 Stock up now! \\r\\r\\n   You...  Extremely Negative  \n",
       "24163  Lysol Laundry Sanitizer Additive Crisp Linen -...             Neutral  \n",
       "22002  Interesting. The sub-saharan story is incomple...            Positive  \n",
       "10475                       This is absolutely THE ONE               Neutral  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Corona_NLP_Train.csv\", encoding=\"latin1\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "466401af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neutral' 'Positive' 'Extremely Negative' 'Negative' 'Extremely Positive']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "class_labels = df[\"Sentiment\"].unique()\n",
    "print(class_labels)\n",
    "print(len(df[\"Sentiment\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e5942ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Extremely Negative\n"
     ]
    }
   ],
   "source": [
    "label_to_index = {}\n",
    "index_to_label = {}\n",
    "\n",
    "for i, label in enumerate(class_labels):\n",
    "    label_to_index[label] = i\n",
    "    index_to_label[i] = label\n",
    "\n",
    "print(label_to_index[\"Extremely Negative\"])\n",
    "print(index_to_label[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c56964bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rubles', 'propertyprices', 'congregate']\n",
      "35441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_12968\\2528759162.py:10: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  print(random.sample(filtered_vocab,3))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "tweets = list(df[\"OriginalTweet\"])\n",
    "cleaned_tweets = [re.sub(r'[^A-Za-z0-9 ]+', '', tweet) for tweet in tweets]\n",
    "# print(cleaned_tweets[100:110])\n",
    "# print(tweets[0:5])\n",
    "vocab= set(\" \".join(cleaned_tweets).split())\n",
    "filtered_vocab = {word for word in vocab if word.islower() and word.isalpha()}\n",
    "print(random.sample(filtered_vocab,3))\n",
    "print(len(filtered_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b15b235",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8e31a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26666\n"
     ]
    }
   ],
   "source": [
    "token_to_word = {}\n",
    "word_to_token = {}\n",
    "for i, word in enumerate(filtered_vocab):\n",
    "    token_to_word[i]=word\n",
    "    word_to_token[word]=i\n",
    "\n",
    "token_to_word[-1]=\"\"\n",
    "word_to_token[\"\"]=-1\n",
    "print(word_to_token[\"several\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d884a65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapid delivery food order made (since no slots elsewhere for weeks). All seemed fine until email listing what was out of stock. They are about to deliver...one bottle of orange juice! #coronavirus #panicbuying #whatashitshow\n",
      "['rapid', 'delivery', 'food', 'order', 'made', 'no', 'slots', 'elsewhere', 'for', 'all', 'seemed', 'fine', 'until', 'email', 'listing', 'what', 'was', 'out', 'of', 'they', 'are', 'about', 'to', 'bottle', 'of', 'orange']\n"
     ]
    }
   ],
   "source": [
    "sample_input = tweets[101]\n",
    "print(sample_input)\n",
    "sample_input = list(sample_input.split())\n",
    "sample_input = [word.lower() for word in sample_input if word.isalpha()]\n",
    "print(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "400d181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34288, 29586, 32267, 8209, 9123, 12143, 34732, 1008, 3889, 6094, 460, 3792, 21853, 4040, 5546, 29832, 15932, 19885, 785, 3598, 21879, 27637, 18298, 11082, 785, 13249]\n",
      "[34288, 29586, 32267, 8209, 9123, 12143, 34732, 1008, 3889, 6094, 460, 3792, 21853, 4040, 5546, 29832, 15932, 19885, 785, 3598, 21879, 27637, 18298, 11082, 785, 13249, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "(50, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tokenized_input = []\n",
    "for word in sample_input:\n",
    "    tokenized_input.append(word_to_token[word])\n",
    "\n",
    "\n",
    "print(tokenized_input)\n",
    "while len(tokenized_input)<50:\n",
    "    tokenized_input.append(-1)\n",
    "\n",
    "print(tokenized_input)\n",
    "tokenized_input = np.array(tokenized_input).reshape(-1,1)\n",
    "print(tokenized_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5ad3b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rapid delivery food order made no slots elsewhere for all seemed fine until email listing what was out of they are about to bottle of orange                        \n"
     ]
    }
   ],
   "source": [
    "reconstructed_input = []\n",
    "tokenized_list_input = tokenized_input.flatten().tolist()\n",
    "# print(tokenized_list_input)\n",
    "for token in tokenized_list_input:\n",
    "    reconstructed_input.append(token_to_word[token])\n",
    "\n",
    "reconstructed_input = \" \".join(reconstructed_input)\n",
    "print(reconstructed_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf3a83",
   "metadata": {},
   "source": [
    "## Embedding + Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71139b26",
   "metadata": {},
   "source": [
    "## Attention (Key, Query, Value dot products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7134d447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34288]\n",
      " [29586]\n",
      " [32267]\n",
      " [ 8209]\n",
      " [ 9123]\n",
      " [12143]\n",
      " [34732]\n",
      " [ 1008]\n",
      " [ 3889]\n",
      " [ 6094]\n",
      " [  460]\n",
      " [ 3792]\n",
      " [21853]\n",
      " [ 4040]\n",
      " [ 5546]\n",
      " [29832]\n",
      " [15932]\n",
      " [19885]\n",
      " [  785]\n",
      " [ 3598]\n",
      " [21879]\n",
      " [27637]\n",
      " [18298]\n",
      " [11082]\n",
      " [  785]\n",
      " [13249]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]\n",
      " [   -1]]\n",
      "(50, 1)\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input)\n",
    "print(tokenized_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19930547",
   "metadata": {},
   "source": [
    "## Add and Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfb5dda",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a28a6f",
   "metadata": {},
   "source": [
    "## Add and Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e2598",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d6995",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a5dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
